ğŸ§  LLM Council â€“ Multi-Agent Decision System

This project implements a minimal LLM Council, where multiple independent AI agents generate answers, and separate judge agents evaluate them using a rubric to produce a single, reliable decision.
The system emphasizes clarity, safety, auditability, and explainability over complexity.

ğŸ“Œ Problem Statement
Single LLM responses can be biased, incomplete, or incorrect.
This project addresses that by using a council-style approach:

Multiple agents generate independent answers

Judges compare answers (without generating new ones)

A final decision is produced with confidence, risks, and citations

All steps are safety-gated and audit-logged


ğŸ¤– Agent Design
Answer Agents (3)

Each answer agent:

Receives the same user prompt

Uses a different system instruction

Runs independently

Does not see other agentsâ€™ outputs

Agent	Behavior
A	Factual and concise
B	Analytical and detailed
C	Cautious and risk-aware

This creates diverse reasoning paths without randomness.

Judge Agents (2)

Judges:

Do NOT generate answers

Compare answers A, B, and C

Use a rubric:

Accuracy

Completeness

Clarity

Risk

Output structured JSON only

ğŸ“Š Decision Object

The final output is a structured Decision Object:

{
  "final_answer": "...",
  "confidence": 1.0,
  "selected_agent": "A",
  "risks": [],
  "citations": [...],
  "judge_votes": ["A", "A"]
}

Confidence

Calculated from judge agreement

Example: 2/2 judges agree â†’ confidence = 1.0

ğŸ” Safety Gating

Input safety check runs before any LLM call

Output safety check scans generated content for risks

Detected risks are propagated into the final decision

ğŸ§¾ Persistent Audit Log

All executions are logged to an append-only JSONL audit log:

Timestamp

User prompt

All agent answers

Judge evaluations

Final decision

ğŸ“ File:

audit_log.jsonl


Why JSONL?
JSON Lines is scalable, append-only, and commonly used in production logging systems.

âš™ï¸ Tech Stack

Python 3

Perplexity API (REST)

Requests

Modular project structure (no frameworks required)

ğŸš€ How to Run
1. Clone the repository
git clone <repo-url>
cd LLM-Council

2. Create and activate virtual environment
python -m venv venv
venv\Scripts\activate   # Windows

3. Install dependencies
pip install -r requirements.txt

4. Set Perplexity API key
setx PERPLEXITY_API_KEY "pplx-your-api-key"


Restart the terminal after this step.

5. Run the system
python main.py

ğŸ“¦ Project Structure
LLM Council/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ answer_agent.py
â”‚   â””â”€â”€ judge_agent.py
â”œâ”€â”€ audit/
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ decision/
â”‚   â””â”€â”€ decision_builder.py
â”œâ”€â”€ safety/
â”‚   â”œâ”€â”€ input_gate.py
â”‚   â””â”€â”€ output_gate.py
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ audit_log.jsonl

ğŸ§  Intentional Non-Automation

Judge conflict resolution is intentionally not automated.

Disagreement between judges is treated as a signal of uncertainty, not an error.
This enables human review at critical decision boundaries and avoids false confidence.

âœ… Scope Notes

This project intentionally avoids:

UI / frontend

Model training

Memory systems

Tool calling

Vector databases

Over-engineering

The focus is clarity and correctness, as requested.

ğŸ Summary

This project demonstrates:

Multi-agent reasoning

Deterministic decision making

Safety-aware orchestration

Audit-ready design

Explainable confidence scoring

It is minimal, extensible, and interview-ready.
